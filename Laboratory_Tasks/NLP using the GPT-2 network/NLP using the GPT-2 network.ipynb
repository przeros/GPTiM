{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<center>\n",
        "    <h1 style = \"font-size:48px; font-weight:normal\"><b>AI</b>-TECH</style></h1>\n",
        "    <br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "    <h1 style = \"font-size:24px; font-weight:normal\">LABORATORIUM 4</style></h1>\n",
        "    <h1 style = \"font-size:24px\">z GŁĘBOKIEGO PRZETWARZANIA TEKSTU I MOWY</style></h1>\n",
        "    <h1 style = \"font-size:24px\">NLP z wykorzystaniem sieci GPT-2</style></h1>\n",
        "    <h1 style = \"font-size:24px\">Przemysław Rośleń</style></h1>\n",
        "    <h3 style = \"font-size:21px\">NLP z wykorzystaniem sieci GPT-2 - przykłady</h3>\n",
        "    <h3 style =\n",
        "</center>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "    </body>\n",
        "    </html>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A96YwI7ZQ8V"
      },
      "source": [
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "    </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsac9OthETth"
      },
      "source": [
        "Instalacja bliblioteki gpt-2-simple pozwalającej na łatwe korzystanie z modelu GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBkpRgBCBS2_",
        "outputId": "1efd4254-ba00-46a1-dfcc-ad3b13602996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE"
      },
      "source": [
        "## Sprawdzenie GPU\n",
        "\n",
        "\n",
        "Colaboratory wykorzystuje karty NVIDIA K80 oraz NVIDIA T4. T4 sprawuje się lepiej jeśli chodzi o trening sieci GPT-2 i posiada więcej pamięci pozwalając na uruchamianie i trening większych modeli GPT-2.\n",
        "\n",
        "\n",
        "Korzystając z poniższej komórki możesz sprawdzić jakie GPU zostało przydzielone przez Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUmTooTW3osf",
        "outputId": "491dc743-18fa-4f78-d19c-6047c3fabffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Nov  1 18:33:39 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdkPnmdnEinW"
      },
      "source": [
        "## Pobieranie GPT-2\n",
        "\n",
        "Istnieją 4 rozmiary modelu GPT-2:\n",
        "\n",
        "* `124M` (standardowy): tzw.  mały model, zajmuje około 500 MB na dysku\n",
        "* `355M`: tzw. średni model, zajmuje 1.5GB na dysku\n",
        "* `774M`: tzw. duży model, nie ma możliwości finetuningu tego modelu z użyciem Colaboratory (ograniczony rozmiar pamięci w dostepnych GPU), ale może być wykorzystany do generowania teksty z pretrenowanego modelu\n",
        "* `1558M`: tzw. super duży model. Współpracuje w Colaboratory tylko z kartami T4, podobnie jak model `774M`, nie ma możliwości zrobienie finetuningu z użyciem Colaboratory.\n",
        "\n",
        "Większe modele potrafią generować lepszy tekst, ale potrzebują więcej czasu do finetuningu i generacji tekstu. Można wybrać jaki model będzie wykorzystany zmieniając wartość parametry `model_name` w komórkach poniżej.\n",
        "Nalezy pamiętać, że model nie jest na stałe przechowywany w Colaboratory. Nalezy ponownie go ściągąć przy kolejnym re-treningu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8wSlgXoDPCR",
        "outputId": "b29e5d3e-94bc-4ff2-eed2-1077a46ef485"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 226Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 5.14Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 600Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:11, 43.4Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 223Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 5.75Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 6.70Mit/s]\n"
          ]
        }
      ],
      "source": [
        "gpt2.download_gpt2(model_name=\"124M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12bGjlVMGvI_"
      },
      "source": [
        "## Montowanie Dysku Google'a\n",
        "\n",
        "Najprostszym sposobem na wczytwanie danych wejściowych do modelu i przechowywanie trenowanych modeli i wyjść jest zamontowanie Dysku Google'a i operowanie plikami z niego.\n",
        "\n",
        "Uruchomienie poniższej komórki zamontuje Dysk Google'a zalogowanego konta, który może być wykorzystany do operowania plikami w Colaboratory. Dodatkowo pojawi się pytanie do kod uwierzytelniający."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be69a524-c06a-4ef6-d388-2b6c6c10369d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "gpt2.mount_gdrive()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT__brhBCvJu"
      },
      "source": [
        "## Ładowanie plików do Colaboratory\n",
        "\n",
        "\n",
        "Załaduj jakikolwiek **mały plik tekstowy** (<10 MB) ,a następnie dokonaj zmiany nazwy pliku zgodnie z załadowanym plikiem w oknie poniżej i uruchom komórkę\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OFnPCLADfll"
      },
      "outputs": [],
      "source": [
        "file_name = \"text.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE"
      },
      "source": [
        "Jeśli plik tekstowy, który chcesz załadować ma rozmiar większy niż 10MB najlepiej wykorzystać do tego celu Dysk Google'a i skopiować z dysku plik prosto do Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z6okFD8VKtS"
      },
      "outputs": [],
      "source": [
        "gpt2.copy_file_from_gdrive(file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "## Finetuning modelu GPT-2\n",
        "\n",
        "\n",
        "Następna komórka rozpocznie rzeczywisty finetuning GPT-2. Tworzona jest trwała sesja Tensorflow, która przechowuje konfigurację treningową, a następnie uruchamia trening dla określonej liczby kroków-epok ukrytych pod parametrem `Steps`. (Aby finetuning trwał w nieskończoność, ustaw `steps = -1`)\n",
        "\n",
        "Domyślnie checkpointy są zapisywane w `/checkpoint/test1` . Są one zapisywane co 500 epok-kroków i gdy komórka zostanie zatrzymana.\n",
        "\n",
        "Trening sieci może trwać powyżej 4 godzin! - Upewnij się, że gdy kończysz trening to zapisujesz wyniki, inaczej stracisz je!\n",
        "\n",
        "**WAŻNA INFORMACJA:** Jeśli chcesz ponownie uruchomić tę komórkę , **najpierw zrejstartuj Colaboratory** (Środowsiko wykonawcze -> Uruchom ponownie Środowsiko wykonawcze). Konieczne będzie ponowne uruchmienie importów, ale nie bęzie koniecznośći ponownego kopiowania plików.\n",
        "\n",
        "Inne opcjonalne parametry do finetuningu GPT-2 (`gpt2.finetune`) z wyjaśnieniem :\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Ustaw wartość na `fresh` aby zaćżąć trening od początku (bazowy model GPT-2) lub `latest` by rozpocząć trening od najnowszego istniejącego checkpointu.\n",
        "* **`sample_every`**: Ilość korków-epok po których następuje wyświetlenie przykładowoego generowanego tekstu przez sieć\n",
        "* **`print_every`**: Ilość epok-kroków po których następuje wyświetlenie postępu treningu\n",
        "* **`learning_rate`**:  Learning rate wykorzystywany do treningu. (standardowo`1e-4`, można obniżyć do `1e-5` w przypadku danych wejściowych poniżej <1MB )\n",
        "*  **`run_name`**: podfolder w którym zapisywany jest checkpoint z modelem subfolder within `checkpoint` to save the model. Jest to przydatne szczególnie podczas pracy z wieloma modelami (konieczne jest podanie `run_name` przy ładowaniu modelu)\n",
        "* **`overwrite`**: Ustaw wartość na `True` jeśli chcesz kontunować finetuning istniejącego modelu(gdy ustawione jest/ `restore_from='latest'`) bez tworzenia duplikatów."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "743516d9-79c3-4c12-cd50-4fdbe2dbdacd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-50acc28782cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m               \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m               \u001b[0msample_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m               \u001b[0msave_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m               )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(sess, dataset, steps, model_name, model_dir, combine, batch_size, learning_rate, accumulate_gradients, restore_from, run_name, checkpoint_dir, sample_every, sample_length, sample_num, multi_gpu, save_every, print_every, max_checkpoints, use_memory_saving_gradients, only_train_transformer_layers, optimizer, overwrite, reuse)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mgpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_available_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     loss = tf.reduce_mean(\n\u001b[1;32m    200\u001b[0m         input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/model.py\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(hparams, X, past, scope, gpus, reuse)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         wpe = tf.compat.v1.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n\u001b[0;32m--> 189\u001b[0;31m                              initializer=tf.compat.v1.random_normal_initializer(stddev=0.01))\n\u001b[0m\u001b[1;32m    190\u001b[0m         wte = tf.compat.v1.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n\u001b[1;32m    191\u001b[0m                              initializer=tf.compat.v1.random_normal_initializer(stddev=0.02))\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1630\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1632\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1340\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1342\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    595\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    547\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     synchronization, aggregation, trainable = (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0;31m# ResourceVariables don't have an op associated with so no traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_variable_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResourceVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0;31m# Throw away internal tf entries and only take a few lines. In some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Variable model/wpe already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?"
          ]
        }
      ],
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='124M',\n",
        "              steps=1000,\n",
        "              restore_from='fresh',\n",
        "              run_name='test1',\n",
        "              print_every=1,\n",
        "              sample_every=10,\n",
        "              save_every=500\n",
        "              )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K"
      },
      "source": [
        "Gdy model jest wytrenowany możesz skopiować checkpoint do swojego Dysku Google'a.\n",
        "\n",
        "Jeśli chcesz pobrać checkpoint na dysk twardy jest rekomendowane, aby najpier skopiować go najpierw na Dysk Google'a, a dopiero z niego pobierać go na komputer. Checkpointy są kompresowane jako archiwa `.rar`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHdTL8NDbAh3"
      },
      "outputs": [],
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='test1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L"
      },
      "source": [
        "## Ładowanie Checkpointa wytrenowanaego modelu\n",
        "\n",
        "Uruchomienie następnej komórki skopiuje plik  `.rar` z wcześniej wytrenowanym checkpointem z Dysko Google'a do Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCcx5u7sbPTD"
      },
      "outputs": [],
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='test1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTa6zf3e_9gV"
      },
      "source": [
        "Następna komórka pozwoli na załatowanie retrenowanego modelu i metadanych niezbędnych do generowania tekstu.\n",
        "\n",
        "**WAŻNA INFORMACJA:** Jeśli chcesz ponownie uruchomić tę komórkę , **najpierw zrejstartuj Colaboratory** (Środowsiko wykonawcze -> Uruchom ponownie Środowsiko wykonawcze). Konieczne będzie ponowne uruchmienie importów, ale nie bęzie koniecznośći ponownego kopiowania plików."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fxL77nvAMAX"
      },
      "outputs": [],
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='test1',reuse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "## Generowanie tesktu z użyciem wytrenowanego modelu.\n",
        "\n",
        "Po wytrenowniau modelu lub załadowaniu retrenowanego modelu z checkpointu możesz generować tekst. Polecenie `generate` generuje pojedynczy tekst z użyciem załadowanego modelu.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RNY6RBI9LmL"
      },
      "outputs": [],
      "source": [
        "gpt2.generate(sess, run_name='test1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "Jeśli chcesz przekazywać wygenerowany tekst dalej jako zmienną możesz wykorzystać następująca konstrukcję: `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
        "\n",
        "Możesz również przekazać parametr `prefix` funkcji generującej zmuszając model, aby generowany tekst zaczynał się od zadanej sekwencji.\n",
        "\n",
        "Można również generować wiele tekstów jednocześnie poprzez parametr `nsamples`.\n",
        "W GPT-2 możliwe jest przekazanie wartośći parametru `batch_size` , aby generować wiele przykładów jednoćżenie uzyskując jednocześnie znaczące przyspieszenie genrowania tesktu ( w przypadku Colabaoratory maksymalna wartość parametru `batch_size` to 20)\n",
        "\n",
        "\n",
        "Inne opcjonalne parametry, które można wykorzystać w poleceniu `gpt2.generate`:\n",
        "\n",
        "*  **`length`**: Liczba generowanych tokenów (domyślnie 1023, wartość maksymalna)\n",
        "* **`temperature`**: Im wyższa wartość tego parametru tym dziwniejsze można otrzymać wyniki w postaci tekstu (standardowo wartość to 0.7, rekomenduje się ustawianie na wartośc między 0.7 a 1.0 )\n",
        "* **`top_k`**: Ograniacza generowane teksty to najlepszych  *k* wskazań. (domyślnie przyjmuje wartość 0, co oznacza, że wyłącza tę funkcję)\n",
        "* **`top_p`**:  Tzw. Nucleus sampling: ograniczna generowane wskazania do skumolowanego prawdopodobieństwa.\n",
        "* **`truncate`**: Obcina tekst wejściowy do danej sekwencji, z wyłączeniem tej sekwencji (np. Jeśli `Truncate = '<| EndoftText |>'`, zwrócony tekst będzie zawierać wszystko przed pierwszym `<|endoftext|>`). Przydatne może być połączenie tego z mniejszą wartością parametru `length`, jeśli teksty wejściowe są krótkie.\n",
        "*  **`include_prefix`**: Jeśli używasz  `truncate` i `include_prefix=False`, wskazany `prefix` nie będzi euzględniony w zwracanym tekście."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DKMc0fiej4N"
      },
      "outputs": [],
      "source": [
        "gpt2.generate(sess,\n",
        "              run_name='test1',\n",
        "              length=250,\n",
        "              temperature=0.7,\n",
        "              prefix=\"I was very bored\",\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2"
      },
      "source": [
        "W przypadku generowania hurtowego można wygenerować dużą ilość tekstu do pliku i sortować przykłady lokalnie na komputerze. Kolejna komórka generuję plik tekstowy z unikalnym znacznikiem czasu.\n",
        "\n",
        "Można uruchamiać poniższą komórkę wiele razy, aby uzyskać jak najwięcej generowanego tekstu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fa6p6arifSL0"
      },
      "outputs": [],
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-LRex8lfv1g"
      },
      "outputs": [],
      "source": [
        "# można uruchomić dwa razy aby ściągnąć wygenerowany plik\n",
        "files.download(gen_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQAN3M6RT7Kj"
      },
      "source": [
        "## Generowanie tekstu z użyciem pretrenowanego modelu\n",
        "\n",
        "\n",
        "Jeśli chcesz wygenerować tekst z użyciem pretrenowanego modelu, a nie modelu poddanego finetuningowi przekaż argument `model_name` zawierający nazwę pretrenowanego modleu do `gpt2.load_gpt2()` i`gpt2.generate()`.\n",
        "\n",
        "Obecnie jest to jedyny sposób aby generować tekst z użyciem modeli 774M i 1558M w Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsUd_jHgUZnD"
      },
      "outputs": [],
      "source": [
        "model_name = \"774M\"\n",
        "\n",
        "gpt2.download_gpt2(model_name=model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAe4NpKNUj2C"
      },
      "outputs": [],
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.load_gpt2(sess, model_name=model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xInIZKaU104"
      },
      "outputs": [],
      "source": [
        "gpt2.generate(sess,\n",
        "              model_name=model_name,\n",
        "              prefix=\"The secret of life is\",\n",
        "              length=100,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}