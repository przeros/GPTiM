{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hF5mw9VwwwB"
      },
      "source": [
        "##### AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Programu Operacyjnego Polska Cyfrowa na lata 2014-2020\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TThfg_eTwwwC"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<center>\n",
        "    <h1 style = \"font-size:48px; font-weight:normal\"><b>AI</b>-TECH</style></h1>\n",
        "    <br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "    <h1 style = \"font-size:24px; font-weight:normal\">LABORATORIUM 4</style></h1>\n",
        "    <h1 style = \"font-size:24px\">z Głębokiego przetwarzanie tekstu i mowy</style></h1>\n",
        "    <h1 style = \"font-size:24px\">NLP z wykorzystaniem sieci GPT\n",
        "2 lub GPT 3 , cz . 1</style></h1>\n",
        "    <h1 style = \"font-size:24px\">Przemysław Rośleń</style></h1>\n",
        "    <h3 style =\n",
        "</center>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "    </body>\n",
        "    </html>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnXIz0htwwwD"
      },
      "source": [
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "    </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUudoY1gwwwD"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a3Kq6E0wwwD"
      },
      "source": [
        "# Analiza struktury algorytmu generowania tekstu przez GPT-2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avIRm1anwwwE"
      },
      "source": [
        "Niniejszy notatnik napisany jest tak, aby wyeksponować procesy tokenizacji oraz konwersji wyjścia sieci GPT-2 do postaci czytelnej dla człowieka."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOf_VojQwwwE",
        "outputId": "912dd2e4-adde-42ba-a13a-6e408b87b76b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.35.1-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.19.1-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.1/311.1 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.1\n"
          ]
        }
      ],
      "source": [
        "# Instalacja biblioteki transformers - domyślnie niestety maszyny Colaba nie posiadają jej zainstalowanej:\n",
        "\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDBXZxQSwwwF"
      },
      "outputs": [],
      "source": [
        "# Import bibliotek\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
        "from transformers import pipeline, set_seed\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# Przydatne źródła:\n",
        "# https://data-dive.com/finetune-german-gpt2-on-tpu-transformers-tensorflow-for-text-generation-of-reviews\n",
        "# https://github.com/huggingface/transformers/issues/2439"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH1GJ4i8wwwF"
      },
      "source": [
        "# Definicja modeli:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX4h2esOwwwG"
      },
      "source": [
        "W razie, gdy modelu nie ma na maszynie - zostanie on pobrany z serwerów firmy utrzymującej bibliotekę transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tXmT57DwwwG",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Do wyboru są różne rozmiary modelu GPT-2:\n",
        "# \"gpt2\"\n",
        "# \"gpt2-medium\"\n",
        "# \"gpt2-large\"\n",
        "# \"gpt2-xl\"\n",
        "# \"distilgpt2\" - wersja kompaktowa uzyskana w wyniku procesu tzw. destylacji wiedzy\n",
        "\n",
        "model     = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_vocabulary('.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncfTQONOX7hL",
        "outputId": "4410c407-4d16-4e01-da2d-f159ae4b9b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./vocab.json', './merges.txt')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iye6Sa-bwwwH"
      },
      "source": [
        "# Przetwarzanie tekstu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkeXvnp9wwwH",
        "outputId": "581ecd3f-cc3f-4214-d44e-f9743d0dd470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded_input:\n",
            "{'input_ids': <tf.Tensor: shape=(1, 13), dtype=int32, numpy=\n",
            "array([[2061,  318,  262, 3280,  284, 1204,   11,  262, 6881,   11,  290,\n",
            "        2279,   30]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 13), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
            "tokenized_outputs\n",
            "tf.Tensor(\n",
            "[[ 2061   318   262  3280   284  1204    11   262  6881    11   290  2279\n",
            "     30   921   423  7147  1204    11   290   674 13532   423  2957   514\n",
            "    612    13   887   644   286  1842    11   286  1842   290  1842    30\n",
            "   6350   561 12157  6486    11   284  4691   534   898  5353    11   290\n",
            "   2251 12157    30  3894  1399   810   561  1793  1842    30  1867   338\n",
            "    534  3572    30   198   198  1639   743  3505    25   618   356   651\n",
            "    477   428  1978    11   356  1183  2245  3612   546  1123   584    13\n",
            "    775  1183  2245  2045   379   674  2460   290  4172   355  2147   517\n",
            "    621   326    13   775]], shape=(1, 100), dtype=int32)\n",
            "Odpowiedź modelu:\n",
            "What is the\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "# Tekst do przetworzenia przez wybraną wersję sieci GPT-2\n",
        "text = \"What is the answer to life, the universe, and everything?\"\n",
        "\n",
        "# Tokenizacja tekstu wejściowego\n",
        "encoded_input     = tokenizer(text, padding=False, add_special_tokens=False, return_tensors='tf')\n",
        "\n",
        "print('encoded_input:')\n",
        "print(encoded_input)\n",
        "\n",
        "# Opcja do_sample powoduje, że następne słowa są wybierane losowo na podstawie rozkładu prawdopodobieństwa zwróconego przez\n",
        "# sieć neuronową - ciekawy efekt można zaobserwować gdy tę losowość wyłączyt się ustawiając do_sample na False.\n",
        "tokenized_outputs = model.generate(input_ids=encoded_input['input_ids'], max_length = 100, do_sample=True)\n",
        "\n",
        "print(f'tokenized_outputs')\n",
        "print(tokenized_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dekodowanie wyjścia z modelu.\n",
        "decoded_answer    = tokenizer.decode(tokenized_outputs[0],\n",
        "                            skip_special_tokens=True,\n",
        "                            clean_up_tokenization_spaces=True)\n",
        "\n",
        "print(\"Odpowiedź modelu:\")\n",
        "print(decoded_answer)\n",
        "print(len(decoded_answer.split(' ')))"
      ],
      "metadata": {
        "id": "dFmJaqd_bYJ4",
        "outputId": "0607c78d-ac35-40dd-d675-2b5cf8463dbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Odpowiedź modelu:\n",
            "What is the answer to life, the universe, and everything? You have chosen life, and our paths have led us there. But what of love, of love and love? Where would happiness lie, to serve your own interests, and create happiness? Well… where would God love? What's your choice?\n",
            "\n",
            "You may remember: when we get all this together, we'll stop thinking about each other. We'll stop looking at our friends and families as nothing more than that. We\n",
            "77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dekodowanie wyjścia z modelu.\n",
        "decoded_answer    = tokenizer.decode(tokenized_outputs[0][0:8],\n",
        "                            skip_special_tokens=True,\n",
        "                            clean_up_tokenization_spaces=True)\n",
        "\n",
        "print(\"Odpowiedź modelu:\")\n",
        "print(decoded_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwOEHnsObBMr",
        "outputId": "91109dbb-ebf6-4a0f-92fe-d0cc4a009a84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Odpowiedź modelu:\n",
            "What is the answer to life, the\n",
            "7\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}